# -*- coding: utf-8 -*-
"""successful_fine_tune_earnings_with_trainer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xDggEE8WQt6Vb8AeRzuZIOPWCf0XUCmC
"""

!pip install transformers
!pip install datasets
!pip install pandas
!pip install tokenizers
!pip install xmltodict
!pip install torch

from google.colab import drive
drive.mount('/content/drive')

#!/usr/bin/env python
import xmltodict
import datasets
from datasets import Dataset, concatenate_datasets
import glob
import pandas as pd
import re



dataframe_list = []
for file in glob.glob("/content/drive/MyDrive/all_earnings_sub/*.xml"):
  #print(file)
  with open(file, 'r', encoding='utf-8') as file: #'pubmed_1_copy.xml'
      my_xml = file.read()
      clean_xml = re.sub("[&;]","",my_xml)
      xml_dict = xmltodict.parse(my_xml)
      df1 = pd.DataFrame(xml_dict, columns=xml_dict.keys())
      order_dict = df1["rst"].iloc[0]
      segments_list_of_dicts = order_dict["segment"]
      xml_dataframe = pd.DataFrame.from_dict(segments_list_of_dicts, orient='columns')
      selected_df = xml_dataframe[~xml_dataframe.isnull().any(axis=1)] #removes null
      selected_df.sort_values(by='@id',ascending=True)
      df_formatted = selected_df.assign(parent_text=selected_df["#text"].shift(1))
      df_formatted["parent_text"].fillna(value=df_formatted.iloc[0]["#text"],inplace=True) #fillna parent
      dataframe_list.append(df_formatted)
      #df_formatted["parent_text"].fillna(value="Introduction",inplace=True) #fillna parent
      dataframe_list.append(df_formatted)
      all_earnings_sub_df = pd.concat(dataframe_list)

print(all_earnings_sub_df.head(15))
#print(set(all_earnings_sub_df['@relname']))

print(len(all_earnings_sub_df))

""" df.drop(df[df['Fee'] >= 24000].index, inplace = True) """
# df2 = df[df.Fee >= 24000]

#no_circumstance = all_earnings_sub_df[all_earnings_sub_df["@relname"] != "circumstance"]
#no_circumstance_result = no_circumstance[no_circumstance["@relname"] != "result"]
print(all_earnings_sub_df["@relname"].value_counts())

import pandas as pd
#l = ['michael','michael','alice','carter']
#pd.Series(l).astype('category').cat.codes.values

rel_labels = list(all_earnings_sub_df["@relname"])
rel_2_int = pd.Series(rel_labels).astype('category').cat.codes.values
print(len(set(rel_2_int)))
#print(label_2_int)

import pandas as pd
#l = ['michael','michael','alice','carter']
#pd.Series(l).astype('category').cat.codes.values

rel_labels = list(all_earnings_sub_df["@relname"])
rel_2_int = pd.Series(rel_labels).astype('category').cat.codes.values
print(len(set(rel_2_int)))
#print(label_2_int)

#hr_df['candidates'] = candidates

all_earnings_sub_df["int_relation"] = rel_2_int.astype(float)
#no_circumstance_result["int_relation"] = rel_2_int.astype(int)

#no_circumstance_result.head(5)

print(all_earnings_sub_df["@relname"].value_counts())

#hr_df['candidates'] = candidates

#all_earnings_sub_df["int_relation"] = rel_2_int.astype(float)
all_earnings_sub_df["int_relation"] = rel_2_int.astype(int)

all_earnings_sub_df.head(5)

print(all_earnings_sub_df["@relname"].value_counts())

"""circumstance    216
result          120
elaboration      76
span             70
condition        60
concession       46
preparation      40
antithesis       36
evaluation       34
cause            20
purpose          18
motivation       18
background       16
solutionhood     12
justify          12
restatement      12
sequence         10
evidence          8
joint             8
Name: @relname, dtype: int64
"""

import datasets
from datasets import Dataset
data_to_dset = all_earnings_sub_df[["#text","parent_text","int_relation"]]

#data_to_dset = no_circumstance_result[["#text","parent_text","int_relation"]]

raw_datasets = Dataset.from_pandas(data_to_dset)

raw_datasets.rename_column("#text","text")
raw_datasets.train_test_split(test_size=0.2)

print(raw_datasets["#text"])

""" tokenize the whole dataset """
from transformers import AutoModel, BertTokenizer

model_ckpt = "reubenraff/earny_bert"
earnings_model = AutoModel.from_pretrained(model_ckpt)
earnings_tokenizer = BertTokenizer.from_pretrained(model_ckpt)

import torch
import torch.nn as nn
from transformers import BertModel, BertConfig, BertForSequenceClassification

class BertForSequenceClassification(nn.Module):

    def __init__(self, config, num_labels=2):
        super(BertForSequenceClassification, self).__init__()
        self.num_labels = num_labels
        self.config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,
        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072,hidden_dropout_prob=0.2)
        self.bert = BertModel.from_pretrained('reubenraff/earny_bert')
        self.dropout = nn.Dropout(self.config.hidden_dropout_prob)
        self.classifier = nn.Linear(self.config.hidden_size, num_labels)
        nn.init.xavier_normal_(self.classifier.weight)
    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):
        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)

        return logits

import torch
text = "why is Pelosi going to Taiwan right now"
zz = earnings_tokenizer.tokenize(text)
tokens_tensor = torch.tensor([earnings_tokenizer.convert_tokens_to_ids(zz)])

outputs = earnings_model(tokens_tensor)
outputs

#ce_loss = nn.CrossEntropyLoss()

from torch import nn
from transformers import Trainer

class MultiClassTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.get("labels")
        outputs = model(**inputs)
        logits = outputs.get('logits')
        loss_fct = nn.CrossEntropy()
        loss = loss_fct(logits.squeeze(), labels.squeeze())
        return (loss, outputs) if return_outputs else loss

from transformers import DataCollatorWithPadding

def tokenize_function(example):
    return earnings_tokenizer(example["#text"], example["parent_text"], truncation=True)

#this is the sentence pair tokenization ^^
tokenized_earnings_dataset = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=earnings_tokenizer)

#print(tokenized_datasets["input_ids"])

#input_ids = bio_tokenizer(sentence,truncation=True, add_special_tokens=True,max_length=512)['input_ids']

wordpieces = [earnings_tokenizer.decode(input_id) for input_id in tokenized_earnings_dataset["input_ids"]]
print(wordpieces)

tokenized_earnings_dataset

import torch

text = "this is a test"
inputs = earnings_tokenizer(text, return_tensors="pt")
print(inputs['input_ids'].size())

inputs = {k:v for k,v in inputs.items()}
with torch.no_grad():
  outputs = earnings_model(**inputs)
print(outputs.last_hidden_state[:,0])
outputs.last_hidden_state[:,0].size()

#print(tokenized_earnings_dataset["input_ids"])

tokenized_earnings_dataset
clean_tokenized_earnings = tokenized_earnings_dataset.remove_columns(["#text", "parent_text", "__index_level_0__"])
earnings_new_names_ds = clean_tokenized_earnings.rename_column('int_relation',"label")
earnings_new_names_ds = earnings_new_names_ds.remove_columns(["token_type_ids"])
print(earnings_new_names_ds)

earnings_new_names_ds.set_format("torch")
#earnings_dataset = earnings_new_names_ds.remove_columns(["token_type_ids"])
#tokenized_datasets.column_names
#renamed_dataset = tokenized_datasets.rename_column("int_relation","label")

tokenized_earnings_encoded = earnings_new_names_ds.train_test_split(test_size=0.2)
tokenized_earnings_encoded

shuffled_encoded = tokenized_earnings_encoded.shuffle(seed=42)

shuffled_encoded

def extract_hidden_states(batch):
  #place model inputs on the GPU
  inputs = {k:v for k,v in batch.items() if k in earnings_tokenizer.model_input_names}
  print(inputs)
  with torch.no_grad():
    last_hidden_state = earnings_model(**inputs) #.last_hidden_state
    print(last_hidden_state)
  return({"hidden_state": last_hidden_state[:,0]})

"""
tokenized_earnings_encoded.set_format("torch",columns=["input_ids","attention_mask","label"])
sent_a = "time flies like an arrow"
sent_b = "fruit flies like a bat"

inputs = earnings_tokenizer(sent_a, sent_b, return_tensors="pt")
hidden_states = earnings_model(**inputs).last_hidden_state
hidden_states
"""

tokenized_earnings_encoded.set_format("torch",columns=["input_ids", "attention_mask","label"])

earnings_hidden = tokenized_earnings_encoded.map(extract_hidden_states,batched=True)

#earnings_hidden = tokenized_earnings_encoded.map(extract_hidden_states, batched=True)

import numpy as np
X_train = np.array(tokenized_earnings_encoded["train"]["input_ids"])
X_valid = np.array(tokenized_earnings_encoded["test"]["input_ids"])
y_train = np.array(tokenized_earnings_encoded["train"]["label"])
y_valid = np.array(tokenized_earnings_encoded["test"]["label"])

from sklearn.dummy import DummyClassifier

dummy_clf = DummyClassifier(strategy="most_frequent")
dummy_clf.fit(X_train,y_train)
dummy_clf.score(X_valid,y_valid)

#tokenized_datasets = tokenized_datasets.remove_columns(["#text", "parent_text", "__index_level_0__"])
#tokenized_datasets = tokenized_datasets.rename_column("int_relation", "labels")
#tokenized_datasets.set_format("torch",columns=["input_ids","attention_mask","label"])
#tokenized_datasets.column_names

#tokenized_datasets.remove_column("#text")
#tokenized_datasets.remove_column("#text")
#tokenized_datasets.rename_column("int_relation","label")
#input_ids, attention_mask, token_type_ids, labels
#print(len(tokenized_datasets["int_relation"]))
#print(len(set(tokenized_datasets["int_relation"])))
#tokenized_datasets

#tokenized_earnings_dict = tokenized_datasets.train_test_split(test_size=0.2)

import torch
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device

from transformers import AutoModelForSequenceClassification


earnings_seq_model = AutoModelForSequenceClassification.from_pretrained(model_ckpt,problem_type="single_label_classification",num_labels=19).to(device)

earnings_seq_model

from sklearn.metrics import accuracy_score, f1_score

def compute_metrics(pred):
  labels = pred.label_ids
  preds = pred.predictions.argmax(-1)
  f1 = f1_score(labels, preds, average="weighted")
  acc = accuracy_score(labels,preds)
  return({"accuracy":acc, "f1":f1})

from torch import nn
from transformers import Trainer

class MultiClassTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        #labels = inputs.get("label")
        outputs = earnings_model(**inputs)
        logits = outputs.logits
        loss_fct = nn.CrossEntropyLoss()
        loss = loss_fct(logits.squeeze(), labels.squeeze())
        return (loss, outputs) if return_outputs else loss

"""
from transformers import TrainingArguments, Trainer
batch_size = 128

logging_steps = len(tokenized_earnings_encoded["train"]) // batch_size

model_name = f"{model_ckpt}-finetuned-earnings"
training_args = TrainingArguments(output_dir=model_name,
                                  num_train_epochs=30,
                                  learning_rate=1e-3,
                                  per_device_train_batch_size=batch_size,
                                  per_device_eval_batch_size=batch_size,
                                  weight_decay=0.01,
                                  evaluation_strategy="epoch",
                                  disable_tqdm=False,
                                  optim="adamw_torch",
                                  logging_steps=logging_steps,
                                  log_level="error")
"""

from transformers import TrainingArguments, Trainer
batch_size = 128
model_name = "reubenraff/earny_bert"
logging_steps = len(tokenized_earnings_encoded["train"]) // batch_size

training_args = TrainingArguments(
                                  output_dir=model_name,
                                  adam_beta1=0.9,
                                  push_to_hub=True,
                                  learning_rate=2e-5,
                                  per_device_train_batch_size=4,
                                  per_device_eval_batch_size=4,
                                  num_train_epochs=10,
                                  warmup_steps=5,
                                  weight_decay=0.01,
                                  gradient_accumulation_steps=16,
                                  evaluation_strategy='epoch',
                                  gradient_checkpointing=True,
                                  optim="adamw_hf"
                                  )

tokenized_earnings_encoded["train"]

tokenized_earnings_encoded["test"]

from huggingface_hub import notebook_login
notebook_login()

from transformers import Trainer
import torch
from torch.optim import AdamW

from transformers import AutoModelForSequenceClassification


earnings_seq_model = AutoModelForSequenceClassification.from_pretrained(model_ckpt,problem_type="single_label_classification",num_labels=19).to(device)
#adam_optimizer =

#input_ids, attention_mask, token_type_ids, labels
#MultiClass
trainer = Trainer(model=earnings_seq_model,
                  args=training_args,
                  compute_metrics=compute_metrics,
                  train_dataset=tokenized_earnings_encoded["train"],
                  eval_dataset=tokenized_earnings_encoded["test"],
                  tokenizer=earnings_tokenizer)
#https://huggingface.co/api/models/reubenraff/earny_bert
#create_optimizer = AdamW(earnings_seq_model.parameters(), lr=2e-5),

trainer.train() #this worked but the score was 0.21

from torch.utils.data import DataLoader

train_dataloader = DataLoader(
    tokenized_earnings_encoded["train"], shuffle=True, batch_size=8, collate_fn=data_collator
)
eval_dataloader = DataLoader(
    tokenized_earnings_encoded["test"], batch_size=8, collate_fn=data_collator
)

from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler
from tqdm import tqdm
from torch.nn import CrossEntropyLoss
model = AutoModelForSequenceClassification.from_pretrained("reubenraff/earny_bert", num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)

num_epochs = 3


num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)


progress_bar = tqdm(range(num_training_steps))

model.train()


for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = CrossEntropyLoss()
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

predictions = trainer.predict(tokenized_earnings_encoded["test"])

predictions.metrics

"""example with emotions dataset from the book"""

from datasets import load_dataset

emotions = load_dataset("emotion")

train_ds = emotions["train"]

import pandas as pd

emotions.set_format(type="pandas")
df = emotions["train"][:]
df.head()

emotions.reset_format()

def label_int2str(row):
  return(emotions["train"].features["label"].int2str(row))

df["label_name"] = df["label"].apply(label_int2str)
df.head()

import matplotlib.pyplot as plt

df["label_name"].value_counts(ascending=True).plot.barh()

plt.title("frequency")
plt.show()

""" tokenize the whole dataset """
from transformers import BertTokenizer

model_ckpt = "reubenraff/earny_bert"

tokenizer = BertTokenizer.from_pretrained(model_ckpt)

def tokenize(batch):
  return(tokenizer(batch["text"], padding=True, truncation=True))


emotions_encoded = emotions.map(tokenize, batched=True, batch_size=None)

emotions_encoded = emotions.map(tokenize,batched=True,batch_size=None)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

from transformers import AutoModel

def extract_hidden_states(batch):
  #Place model inputs on the GPU
  model = AutoModel.from_pretrained("distilbert-base-uncased").to(device)
  inputs = {k:v.to(device) for k, v in batch.items() if k in tokenizer.model_input_names}
  #extract last hidden states
  with torch.no_grad():
    last_hidden_state = model(**inputs).last_hidden_state
    return({"hidden_state": last_hidden_state[:,0].cpu().numpy()})

print(emotions_encoded["train"].column_names)

emotions_encoded.set_format("torch",columns=["input_ids", "attention_mask","label"])

emotions_hidden = emotions_encoded.map(extract_hidden_states,batched=True)

import numpy as np

X_train = np.array(emotions_hidden["train"]["hidden_state"])
X_valid = np.array(emotions_hidden["validation"]["hidden_state"])
y_train = np.array(emotions_hidden["train"]["label"])
y_valid = np.array(emotions_hidden["validation"]["label"])

X_train.shape, X_valid.shape

from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import MinMaxScaler

X_scaled = MinMaxScaler().fit_transform(X_train)

lr_clf = LogisticRegression(max_iter=3000)
lr_clf.fit(X_train,y_train)
lr_clf.score(X_valid,y_valid)

from sklearn.dummy import DummyClassifier

dummy_classifier = DummyClassifier(strategy="most_frequent")
dummy_classifier.fit(X_train,y_train)
dummy_classifier.score(X_valid, y_valid)

import torch
from transformers import AutoModelForSequenceClassification

#model_ckpt = "distilbert-base-uncased"

#tokenizer = AutoTokenizer.from_pretrained("reubenraff/earny_bert")

model = AutoModelForSequenceClassification.from_pretrained("reubenraff/earny_bert")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
num_labels = 6
model = AutoModelForSequenceClassification.from_pretrained(model_ckpt,num_labels=num_labels).to(device)

from sklearn.metrics import accuracy_score, f1_score

def compute_metrics(pred):
  labels = pred.label_ids
  preds = pred.predictions.argmax(-1)
  f1 = f1_score(labels, preds, average="weighted")
  acc = accuracy_score(labels,preds)
  return({"accuracy":acc, "f1":f1})

from transformers import TrainingArguments, Trainer
batch_size = 64

logging_steps = len(emotions_encoded["train"]) // batch_size

model_name = f"{model_ckpt}-finetuned-emotion"
training_args = TrainingArguments(output_dir=model_name,
                                  num_train_epochs=2,
                                  learning_rate=2e-5,
                                  per_device_train_batch_size=batch_size,
                                  per_device_eval_batch_size=batch_size,
                                  weight_decay=0.01,
                                  evaluation_strategy="epoch",
                                  disable_tqdm=False,
                                  logging_steps=logging_steps,
                                  log_level="error")

emotions_encoded

from transformers import Trainer


trainer = Trainer(model=model, args=training_args,
                  compute_metrics=compute_metrics,
                  train_dataset=emotions_encoded["train"],
                  eval_dataset=emotions_encoded["validation"],
                  tokenizer=tokenizer)
trainer.train()

trainer.evaluate()