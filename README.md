# master_thesis
Research efforts in transfer learning have gained massive popularity in recent years. Pre-trained language models have demonstrated the most successful results in producing high quality neural networks capable of quality inference after training across domains via transfer learning. This study expands on the domain transfer explored in Ferracane et al. (2019) exploring neural methods for transfer learning of discourse parsing between a news source domain and a medical target domain. Ferracane et al. (2019) specifically discuss transfer learning from news articles to PubMed medical journal articles. Experiments in transfer learning expand to include three domains: Wall Street Journal articles previously annotated with Rhetorical Structure Theory relations, PubMed abstracts, and earnings calls transcripts. BERT pretrained on scientific data, called SciBert Beltagy et al. (2019), is used. Experiments are conducted to finetune SciBert on Wall Street Journal articles and Earnings calls transcripts. The transcripts are annotated through the rstWeb tool (Zeldes 2016) with a subset of RST labels labeling relations between clauses. Results demonstrate progress in transfer learning between distinct domains is extremely challenging. There are multiple avenues for innovation and improvement to explore. In-domain training where the pretrained model domain matches the domain of the fine-tuned data yielded better results.
